{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJmn7AiJiKHjGvniiBceTN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/piyushjaangid/ImageEditWebApp/blob/main/Travel_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGhhQehc3JUC"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Import libraries for NLP and text preprocessing\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Import machine learning models\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Import deep learning models\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Import BERT\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "\n",
        "# -----------------------------------\n",
        "# Load and explore the dataset\n",
        "# -----------------------------------\n",
        "\n",
        "# Replace this with your dataset path or link\n",
        "# Example: Social media posts, traveler feedback, and sentiments labeled as Positive, Neutral, or Negative\n",
        "data = pd.read_csv('traveler_sentiment_data.csv')\n",
        "\n",
        "# Print the first few rows of the dataset to understand its structure\n",
        "print(data.head())\n",
        "\n",
        "# -----------------------------------\n",
        "# Preprocessing the text data\n",
        "# -----------------------------------\n",
        "\n",
        "# Download stopwords from NLTK\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Define a function to clean and preprocess text data\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove stopwords\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "    # Remove special characters and numbers\n",
        "    text = ''.join([char for char in text if char.isalpha() or char.isspace()])\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply the preprocessing function to the text data\n",
        "data['cleaned_text'] = data['text'].apply(preprocess_text)\n",
        "\n",
        "# Encode the sentiment labels (Positive = 2, Neutral = 1, Negative = 0)\n",
        "label_encoder = LabelEncoder()\n",
        "data['sentiment_encoded'] = label_encoder.fit_transform(data['sentiment'])\n",
        "\n",
        "# -----------------------------------\n",
        "# Text Vectorization: Bag of Words or TF-IDF\n",
        "# -----------------------------------\n",
        "\n",
        "# Option 1: Bag of Words (BoW) using CountVectorizer\n",
        "vectorizer = CountVectorizer(max_features=5000)\n",
        "X_bow = vectorizer.fit_transform(data['cleaned_text']).toarray()\n",
        "\n",
        "# Option 2: TF-IDF Vectorizer (for text importance weighting)\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_tfidf = tfidf.fit_transform(data['cleaned_text']).toarray()\n",
        "\n",
        "# Define target variable\n",
        "y = data['sentiment_encoded']\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# -----------------------------------\n",
        "# Model 1: Naive Bayes for Sentiment Classification\n",
        "# -----------------------------------\n",
        "\n",
        "# Initialize the Naive Bayes model\n",
        "nb = MultinomialNB()\n",
        "\n",
        "# Train the Naive Bayes model\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_nb = nb.predict(X_test)\n",
        "\n",
        "# Evaluate the Naive Bayes model performance\n",
        "print(\"Naive Bayes Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_nb))\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix_nb = confusion_matrix(y_test, y_pred_nb)\n",
        "sns.heatmap(conf_matrix_nb, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Naive Bayes Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------------\n",
        "# Model 2: LSTM for Sentiment Classification\n",
        "# -----------------------------------\n",
        "\n",
        "# Tokenization for LSTM (convert text to sequences)\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(data['cleaned_text'])\n",
        "X_lstm = tokenizer.texts_to_sequences(data['cleaned_text'])\n",
        "\n",
        "# Padding the sequences to ensure uniform input length\n",
        "X_lstm = pad_sequences(X_lstm, maxlen=200)  # Adjust maxlen as needed\n",
        "\n",
        "# Split the padded sequences into training and testing sets\n",
        "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(X_lstm, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the LSTM model\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(Embedding(input_dim=5000, output_dim=128, input_length=200))\n",
        "lstm_model.add(Bidirectional(LSTM(128, return_sequences=False)))\n",
        "lstm_model.add(Dense(3, activation='softmax'))  # Output layer with 3 classes (Positive, Neutral, Negative)\n",
        "\n",
        "# Compile the model\n",
        "lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the LSTM model\n",
        "lstm_model.fit(X_train_lstm, y_train_lstm, epochs=5, batch_size=64, verbose=1)\n",
        "\n",
        "# Evaluate the LSTM model on the test set\n",
        "y_pred_lstm = np.argmax(lstm_model.predict(X_test_lstm), axis=-1)\n",
        "\n",
        "# Print LSTM accuracy and classification report\n",
        "print(\"LSTM Accuracy:\", accuracy_score(y_test_lstm, y_pred_lstm))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test_lstm, y_pred_lstm))\n",
        "\n",
        "# -----------------------------------\n",
        "# Model 3: BERT for Sentiment Classification\n",
        "# -----------------------------------\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
        "\n",
        "# Encode the text data for BERT input\n",
        "def encode_for_bert(texts):\n",
        "    return bert_tokenizer(\n",
        "        texts.tolist(),\n",
        "        add_special_tokens=True,\n",
        "        max_length=200,\n",
        "        padding='max_length',\n",
        "        return_attention_mask=True,\n",
        "        truncation=True,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "# Prepare the training and test sets for BERT\n",
        "train_encoded = encode_for_bert(data['cleaned_text'].iloc[X_train.index])\n",
        "test_encoded = encode_for_bert(data['cleaned_text'].iloc[X_test.index])\n",
        "\n",
        "# Train the BERT model\n",
        "bert_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "bert_model.fit(\n",
        "    x={'input_ids': train_encoded['input_ids'], 'attention_mask': train_encoded['attention_mask']},\n",
        "    y=y_train,\n",
        "    epochs=3,\n",
        "    batch_size=32,\n",
        "    validation_data=({'input_ids': test_encoded['input_ids'], 'attention_mask': test_encoded['attention_mask']}, y_test)\n",
        ")\n",
        "\n",
        "# Evaluate the BERT model\n",
        "bert_eval = bert_model.evaluate({'input_ids': test_encoded['input_ids'], 'attention_mask': test_encoded['attention_mask']}, y_test)\n",
        "print(\"BERT Accuracy:\", bert_eval[1])\n",
        "\n",
        "# -----------------------------------\n",
        "# K-Means Clustering for Unsupervised Learning\n",
        "# -----------------------------------\n",
        "\n",
        "# Perform K-Means clustering on the TF-IDF representation of text\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)  # Assume 3 clusters: Positive, Neutral, Negative\n",
        "kmeans.fit(X_tfidf)\n",
        "\n",
        "# Predict cluster labels\n",
        "clusters = kmeans.predict(X_tfidf)\n",
        "\n",
        "# Visualize the distribution of clusters\n",
        "sns.countplot(clusters)\n",
        "plt.title(\"Cluster Distribution (K-Means)\")\n",
        "plt.xlabel(\"Cluster\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ]
}